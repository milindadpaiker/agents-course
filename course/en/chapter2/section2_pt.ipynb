{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ankamse0wScj"
      },
      "source": [
        "# Behind the pipeline (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTJpANZIwScj"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2G1MtjgwSck"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHt_VcozwSck",
        "outputId": "e23c0dc0-5ea1-485f-ffb8-e94da0c3ff53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9598047137260437},\n",
              " {'label': 'NEGATIVE', 'score': 0.9994558095932007}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\n",
        "    [\n",
        "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "        \"I hate this so much!\",\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvYKIXR3wScl"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNFhuJxswScl",
        "outputId": "c9a6d559-1c74-4b35-f764-ebf2b8a1a090"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{\n",
              "    'input_ids': tensor([\n",
              "        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],\n",
              "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]\n",
              "    ]), \n",
              "    'attention_mask': tensor([\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
              "    ])\n",
              "}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDz80_6pwScl"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModel.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGzvaxvIwScl",
        "outputId": "fb8decf4-c779-4568-948d-3d6791b43f8c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 16, 768])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = model(**inputs)\n",
        "print(outputs.last_hidden_state.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtWXgBw-wScl"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCCW39vdwScl",
        "outputId": "f9a9cea9-3796-4104-f696-ac2e8d4a78f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 2])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(outputs.logits.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjSordLlwScm",
        "outputId": "e773c469-9001-453b-fa3f-ec08c088e3be"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1.5607,  1.6123],\n",
              "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(outputs.logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQNORHzhwScm",
        "outputId": "861f568e-81be-422b-abe6-9589c073dd17"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[4.0195e-02, 9.5980e-01],\n",
              "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AC8peC8LwScm",
        "outputId": "50e89f50-d22b-44ad-81b4-6c7d93d7f071"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 'NEGATIVE', 1: 'POSITIVE'}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.config.id2label"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas scikit-learn matplotlib seaborn"
      ],
      "metadata": {
        "id": "iXRRTUJLwTOf",
        "outputId": "4e54a43e-3427-4c5a-ac9c-212b32ac998b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler # Essential for Gradient Descent\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Load the California Housing dataset\n",
        "california_housing = fetch_california_housing(as_frame=True)\n",
        "X = california_housing.data  # Features\n",
        "y = california_housing.target # Target: median house value (in hundreds of thousands of dollars)\n",
        "\n",
        "# 2. Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Crucial for Gradient Descent: Feature Scaling (Standardization) ---\n",
        "# Gradient Descent works best when features are on a similar scale.\n",
        "# StandardScaler transforms data to have zero mean and unit variance.\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test) # Use the same scaler fitted on training data for test data\n",
        "\n",
        "# Convert back to DataFrame for easier handling if needed (optional for core algorithm)\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)"
      ],
      "metadata": {
        "id": "i-oVrhrpxC0I"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(X_train_scaled)"
      ],
      "metadata": {
        "id": "P5GKdzyZxZRE",
        "outputId": "3be35aec-6a3d-4069-890b-53f7de2f8419",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 3. Implement Linear Regression with Gradient Descent from Scratch\n",
        "\n",
        "class LinearRegressionGD:\n",
        "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.weights = None # Coefficients (beta_1 to beta_n)\n",
        "        self.bias = None    # Intercept (beta_0)\n",
        "        self.cost_history = [] # To track MSE over iterations\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Add bias (intercept) term to X\n",
        "        # We add a column of ones to X, so the first weight (beta_0) can be learned\n",
        "        # directly along with other weights.\n",
        "        X_b = np.c_[np.ones((X.shape[0], 1)), X] # Add column of 1s for bias term\n",
        "\n",
        "        n_samples, n_features = X_b.shape # n_features now includes the bias term\n",
        "\n",
        "        # Initialize weights (coefficients) to zeros or small random values\n",
        "        self.weights = np.zeros(n_features) # Includes bias as the first weight\n",
        "        print()\n",
        "\n",
        "        # Reshape y to be a 2D array if it's 1D\n",
        "        y = y.values.reshape(-1, 1) if isinstance(y, pd.Series) else y.reshape(-1, 1)\n",
        "\n",
        "\n",
        "        # Gradient Descent loop\n",
        "        for iteration in range(self.n_iterations):\n",
        "            # Calculate predictions (hypothesis)\n",
        "            y_predicted = X_b.dot(self.weights.reshape(-1, 1))\n",
        "\n",
        "            # Calculate the error (residuals)\n",
        "            errors = y_predicted - y\n",
        "\n",
        "            # Calculate gradients (partial derivatives of cost function w.r.t. each weight)\n",
        "            # Derivative of MSE is (2/m) * sum((y_pred - y_actual) * X_j)\n",
        "            gradients = (2/n_samples) * X_b.T.dot(errors)\n",
        "\n",
        "            # Update weights\n",
        "            self.weights -= self.learning_rate * gradients.flatten() # Flatten gradients to 1D\n",
        "\n",
        "            # Store cost (MSE) for monitoring convergence\n",
        "            cost = np.mean(errors**2) # MSE\n",
        "            self.cost_history.append(cost)\n",
        "\n",
        "        # Separate bias from other weights for clarity\n",
        "        self.bias = self.weights[0]\n",
        "        self.weights = self.weights[1:]\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Add bias term to X for prediction\n",
        "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
        "        # Use learned weights and bias to make predictions\n",
        "        return X_b.dot(np.concatenate(([self.bias], self.weights))).flatten()\n",
        "\n"
      ],
      "metadata": {
        "id": "s72wasUkwUKe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Train and Evaluate the Custom Model ---\n",
        "print(\"--- Training Custom Linear Regression (Gradient Descent) ---\")\n",
        "# Instantiate our custom model\n",
        "model_gd = LinearRegressionGD(learning_rate=0.01, n_iterations=2000) # Increased iterations\n",
        "\n",
        "# Fit the model to the scaled training data\n",
        "model_gd.fit(X_train_scaled, y_train) # Pass numpy arrays, not DataFrames for internal ops\n",
        "\n",
        "print(f\"\\nCustom Model - Intercept (beta_0): {model_gd.bias:.4f}\")\n",
        "print(\"Custom Model - Coefficients (beta_i for each feature):\")\n",
        "for feature, coef in zip(X.columns, model_gd.weights):\n",
        "    print(f\"  {feature}: {coef:.4f}\")\n",
        "\n",
        "\n",
        "# Make predictions on the scaled test data\n",
        "y_pred_gd = model_gd.predict(X_test_scaled)\n",
        "\n",
        "\n",
        "# Evaluate the custom model's performance\n",
        "mse_gd = mean_squared_error(y_test, y_pred_gd)\n",
        "rmse_gd = np.sqrt(mse_gd)\n",
        "r2_gd = r2_score(y_test, y_pred_gd)\n",
        "\n",
        "print(\"\\n--- Custom Model Evaluation on Test Set ---\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_gd:.4f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse_gd:.4f}\")\n",
        "print(f\"R-squared (R2 Score): {r2_gd:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Ve0HvYI3xMWn",
        "outputId": "3866a2ff-fd48-4e2d-bbf2-ceab32d4ccba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training Custom Linear Regression (Gradient Descent) ---\n",
            "\n",
            "Custom Model - Intercept (beta_0): 2.0719\n",
            "Custom Model - Coefficients (beta_i for each feature):\n",
            "  MedInc: 0.8648\n",
            "  HouseAge: 0.1298\n",
            "  AveRooms: -0.3039\n",
            "  AveBedrms: 0.3423\n",
            "  Population: 0.0001\n",
            "  AveOccup: -0.0418\n",
            "  Latitude: -0.8332\n",
            "  Longitude: -0.8069\n",
            "\n",
            "--- Custom Model Evaluation on Test Set ---\n",
            "Mean Squared Error (MSE): 0.5558\n",
            "Root Mean Squared Error (RMSE): 0.7455\n",
            "R-squared (R2 Score): 0.5759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Visualize Cost History ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(len(model_gd.cost_history)), model_gd.cost_history, color='blue')\n",
        "plt.title(\"Cost History (MSE) during Gradient Descent Training\")\n",
        "plt.xlabel(\"Number of Iterations\")\n",
        "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# --- Visualize Predictions vs. Actual ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test, y_pred_gd, alpha=0.3)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2) # Perfect prediction line\n",
        "plt.xlabel(\"Actual Median House Value ($100,000s)\")\n",
        "plt.ylabel(\"Predicted Median House Value ($100,000s)\")\n",
        "plt.title(\"Actual vs. Predicted House Values (Custom Linear Regression GD)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "BMMfUSzVxVYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Compare with Scikit-learn's Linear Regression (for sanity check) ---\n",
        "from sklearn.linear_model import LinearRegression\n",
        "skl_model = LinearRegression()\n",
        "skl_model.fit(X_train_scaled, y_train) # Fit on scaled data for fair comparison\n",
        "y_pred_skl = skl_model.predict(X_test_scaled)\n",
        "\n",
        "mse_skl = mean_squared_error(y_test, y_pred_skl)\n",
        "r2_skl = r2_score(y_test, y_pred_skl)\n",
        "\n",
        "print(\"\\n--- Scikit-learn Linear Regression (for comparison) ---\")\n",
        "print(f\"Intercept (beta_0): {skl_model.intercept_:.4f}\")\n",
        "print(\"Coefficients (beta_i for each feature):\")\n",
        "for feature, coef in zip(X.columns, skl_model.coef_):\n",
        "    print(f\"  {feature}: {coef:.4f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_skl:.4f}\")\n",
        "print(f\"R-squared (R2 Score): {r2_skl:.4f}\")"
      ],
      "metadata": {
        "id": "wHgFnDY7xWyY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Behind the pipeline (PyTorch)",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}